<!doctype html><html lang='ko'><head><meta charset='UTF-8'/><meta name='viewport' content='width=device-width, initial-scale=1.0'/><title>SD3 / SD3.5 — Architecture</title><link rel='stylesheet' href='../assets/style.css'/><script>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']]},options:{skipHtmlTags:['script','noscript','style','textarea','pre','code']}};</script><script async src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js'></script></head><body><div class='wrap'><article class='paper'>
<p class="small"><a href="../index.html">← Training Master Docs</a> / architecture / sd3</p>
<h1>Stable Diffusion 3 / 3.5 — MMDiT</h1>
<div class="chips">
  <span class="chip">백본: MMDiT (Multi-Modal DiT)</span>
  <span class="chip">확산: Rectified Flow</span>
  <span class="chip">파라미터: 2B / 8B</span>
  <span class="chip">유형: Image</span>
  <span class="chip tag sdscripts">sd-scripts ✓</span>
  <span class="chip tag diffpipe">diffusion-pipe ✓</span>
  <span class="chip tag diffsynth">DiffSynth-Studio ✗</span>
</div>

<h2>MMDiT 아키텍처</h2>
<div class="formula">$$z_0 \xrightarrow{x_t=(1-\sigma_t)z_0+\sigma_t\epsilon} x_t \xrightarrow{\text{MMDiT}_\theta} v_\theta$$</div>

<h3>핵심 혁신: Multi-Modal Attention</h3>
<p>이미지 토큰과 텍스트 토큰이 <strong>동일 어텐션 공간</strong>에서 상호작용. 각 스트림은 독립적 Linear projection → 합산된 QKV로 joint attention → 각 스트림이 독립적 출력 projection.</p>

<div class="formula">
$$\text{Attn}(Q,K,V) = \text{softmax}\!\left(\frac{[Q_\text{img};\,Q_\text{txt}]\cdot[K_\text{img};\,K_\text{txt}]^\top}{\sqrt{d_\text{head}}}\right)\cdot[V_\text{img};\,V_\text{txt}]$$
</div>

<h3>Triple Text Encoder</h3>
<table>
  <thead><tr><th>인코더</th><th>출력 차원</th><th>역할</th></tr></thead>
  <tbody>
    <tr><td><strong>CLIP-L</strong></td><td>$768$</td><td>Pooled + token sequence</td></tr>
    <tr><td><strong>CLIP-G (OpenCLIP)</strong></td><td>$1280$</td><td>Pooled + token sequence</td></tr>
    <tr><td><strong>T5-XXL</strong></td><td>$4096$</td><td>Token sequence only</td></tr>
  </tbody>
</table>
<p class="small">SD3.5 Medium은 CLIP-L + CLIP-G만 사용 (T5 제외). <code>--clip_l</code>, <code>--clip_g</code>, <code>--t5xxl</code> 플래그로 개별 제어.</p>

<h2>Rectified Flow 훈련</h2>
<div class="formula">$$\mathcal{L}_\text{RF}=\mathbb{E}_{z_0,\epsilon,t}\left[\|v_\theta(x_t,t,c)-(\epsilon-z_0)\|^2\right]$$</div>

<h3>Timestep / 시그마 변환</h3>
<div class="formula">$$\sigma(t)=t,\quad x_t=(1-\sigma)z_0+\sigma\epsilon \quad\text{(linear interpolation)}$$</div>
<p>SD3는 <strong>shift=3.0</strong>을 기본 사용: $\sigma'=\frac{s\cdot\sigma}{1+(s-1)\sigma}$</p>

<h2>훈련 모드</h2>
<table>
  <thead><tr><th>모드</th><th>설명</th><th>sd-scripts</th><th>diffusion-pipe</th></tr></thead>
  <tbody>
    <tr><td><strong>Full Fine-tune</strong></td><td>MMDiT 전체 파라미터 업데이트</td><td><code>sd3_train.py</code></td><td>✓</td></tr>
    <tr><td><strong>LoRA</strong></td><td>Low-Rank Adaptation 삽입</td><td><code>sd3_train_network.py</code></td><td>✓</td></tr>
    <tr><td><strong>ControlNet</strong></td><td>조건 분기 학습</td><td>지원 예정</td><td>—</td></tr>
  </tbody>
</table>

<h2>sd-scripts 전용 옵션</h2>
<div class="codebox"># SD3 특수 파라미터
--model_prediction_type=raw        # v_prediction 직접 사용
--timestep_sampling=sigma          # sigma-based 스케줄
--discrete_flow_shift=3.0          # default shift
--weighting_scheme=logit_normal    # 가중치 방식 (none/sigma_sqrt/mode/cosmap/logit_normal)

# 메모리 절약
--learning_rate_te1=0  --learning_rate_te2=0  # TE 동결
--fp8_base                                     # FP8 양자화
--blocks_to_swap=5                             # CPU offload</div>

<h3>Weighting Schemes 비교</h3>
<table>
  <thead><tr><th>방식</th><th>수식</th><th>특징</th></tr></thead>
  <tbody>
    <tr><td><strong>none</strong></td><td>$w(t)=1$</td><td>균일 가중</td></tr>
    <tr><td><strong>sigma_sqrt</strong></td><td>$w(\sigma)=\sqrt{\sigma}$</td><td>고노이즈 강조</td></tr>
    <tr><td><strong>mode</strong></td><td>$w(t)=\frac{1}{\pi\sigma_n}\cdot\frac{1}{e^{(t-\mu)/\sigma_n}+e^{-(t-\mu)/\sigma_n}}$</td><td>logit-normal 모드 중심</td></tr>
    <tr><td><strong>cosmap</strong></td><td>$w(t)=1-\cos^2\!\left(\frac{\pi t}{2}\right)$</td><td>중앙부 강조</td></tr>
    <tr><td><strong>logit_normal</strong></td><td>$w(t)\propto\frac{1}{t(1-t)}\cdot\exp\!\left(-\frac{(\text{logit}(t)-\mu)^2}{2s^2}\right)$</td><td>중앙 집중, 양 끝 억제</td></tr>
  </tbody>
</table>

<h2>LoRA 타겟 모듈</h2>
<div class="codebox"># sd-scripts: networks/lora_sd3.py
# 대상 레이어:
# - context_block.attn.{qkv, proj}     # 텍스트 스트림
# - x_block.attn.{qkv, proj}           # 이미지 스트림
# - context_block.mlp.{fc1, fc2}
# - x_block.mlp.{fc1, fc2}
# - final_layer.*

# 제어 파라미터:
# --network_train_unet_only   (기본: DiT 만 학습)
# --network_train_text_encoder (TE도 학습에 포함)</div>

<h2>권장 설정</h2>
<table>
  <thead><tr><th>설정</th><th>LoRA</th><th>Full FT</th></tr></thead>
  <tbody>
    <tr><td>Learning Rate</td><td>1e-4 ~ 3e-4</td><td>5e-7 ~ 5e-6</td></tr>
    <tr><td>LoRA Rank</td><td>16~64</td><td>—</td></tr>
    <tr><td>timestep_sampling</td><td>sigma (default)</td><td>동일</td></tr>
    <tr><td>discrete_flow_shift</td><td>3.0</td><td>동일</td></tr>
    <tr><td>weighting_scheme</td><td>logit_normal</td><td>logit_normal</td></tr>
    <tr><td>Mixed Precision</td><td>BF16</td><td>BF16</td></tr>
  </tbody>
</table>
</article></div></body></html>
