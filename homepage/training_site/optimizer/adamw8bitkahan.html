<!doctype html><html lang='ko'><head><meta charset='UTF-8'/><meta name='viewport' content='width=device-width, initial-scale=1.0'/><title>AdamW8bitKahan — Training Optimizer</title><link rel='stylesheet' href='../assets/style.css'/><script>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']]},options:{skipHtmlTags:['script','noscript','style','textarea','pre','code']}};</script><script async src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js'></script></head><body><div class='wrap'><article class='paper'>
<p class="small"><a href="../index.html">← Training Master Docs</a> / optimizer / adamw8bitkahan</p>
<h1>AdamW8bitKahan — 8-bit + Kahan + StableAdamW</h1>
<div class="chips">
  <span class="chip">분류: Adam 계열 (양자화+보정)</span>
  <span class="chip tag diffpipe">diffusion-pipe ✓</span>
</div>

<h2>수학 정의</h2>
<p>bitsandbytes의 8-bit 양자화 + optimi의 Kahan Summation + StableAdamW 모드를 결합한 하이브리드.</p>
<div class="formula">
$$\text{8-bit 양자화: }m_t^{(8)}=Q_8(m_t),\; v_t^{(8)}=Q_8(v_t) \quad\text{→ 4x 메모리 절감}$$
$$\text{Kahan 보정: }c_{t+1}=y-\text{round}_\text{BF16}(y) \quad\text{→ BF16 정밀도 보상}$$
$$\text{StableAdamW: }\text{stabilize}=\text{True}\quad\text{→ RMS 정규화 안정화}$$
</div>

<h2>3중 최적화의 의미</h2>
<div class="box">
<p><strong>메모리 (8-bit)</strong>: 옵티마이저 상태 VRAM 4x 절감</p>
<p><strong>정밀도 (Kahan)</strong>: BF16 파라미터의 누적 반올림 오차 보정</p>
<p><strong>안정성 (StableAdamW)</strong>: 큰 기울기에 대한 업데이트 폭발 방지</p>
<p class="small">세 기법이 서로 보완적: 8-bit는 메모리를, Kahan은 양자화 오차를, Stable은 수치 안정성을 처리.</p>
</div>

<h2>코드 매핑</h2>
<div class="codebox"># diffusion-pipe: type = "adamw8bitkahan"
from optimizers.adamw_8bit import AdamW8bitKahan
optimizer = AdamW8bitKahan(
    params, lr=lr, betas=betas,
    stabilize=True  # StableAdamW 모드 활성화
)</div>
</article></div></body></html>
