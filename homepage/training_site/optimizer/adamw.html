<!doctype html><html lang='ko'><head><meta charset='UTF-8'/><meta name='viewport' content='width=device-width, initial-scale=1.0'/><title>AdamW — Training Optimizer</title><link rel='stylesheet' href='../assets/style.css'/><script>window.MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']]},options:{skipHtmlTags:['script','noscript','style','textarea','pre','code']}};</script><script async src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js'></script></head><body><div class='wrap'><article class='paper'>
<p class="small"><a href="../index.html">← Training Master Docs</a> / optimizer / adamw</p>
<h1>AdamW — Decoupled Weight Decay Adam</h1>
<div class="chips">
  <span class="chip">분류: Adam 계열</span>
  <span class="chip tag sdscripts">sd-scripts ✓</span>
  <span class="chip tag diffpipe">diffusion-pipe ✓</span>
  <span class="chip tag diffsynth">DiffSynth-Studio ✓</span>
</div>

<h2>수학 정의</h2>
<p>Loshchilov &amp; Hutter (2019). Weight decay를 gradient update에서 분리한 Adam 변형.</p>
<div class="formula">
$$\begin{aligned}
g_t &= \nabla_\theta \mathcal{L}(\theta_t) \\
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \quad &\text{(1st moment — momentum)} \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \quad &\text{(2nd moment — adaptive LR)} \\
\hat{m}_t &= \frac{m_t}{1-\beta_1^t} \quad &\text{(bias correction)} \\
\hat{v}_t &= \frac{v_t}{1-\beta_2^t} \quad &\text{(bias correction)} \\
\theta_{t+1} &= \theta_t - \eta\left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon} + \lambda\theta_t\right) \quad &\text{(decoupled WD)}
\end{aligned}$$
</div>

<h2>파라미터 상세</h2>
<table>
  <thead><tr><th>파라미터</th><th>기본값</th><th>수식 대응</th><th>튜닝 가이드</th></tr></thead>
  <tbody>
    <tr><td><code>lr</code></td><td>2e-6 (sd-scripts)<br>1e-4 (diffusion-pipe, DiffSynth)</td><td>$\eta$</td><td>LoRA: 1e-4, Full FT: 1e-6~5e-6</td></tr>
    <tr><td><code>betas</code></td><td>(0.9, 0.999)</td><td>$(\beta_1, \beta_2)$</td><td>$\beta_1$↑: 모멘텀 관성 증가. $\beta_2$↑: LR 적응 지연</td></tr>
    <tr><td><code>eps</code></td><td>1e-8</td><td>$\epsilon$</td><td>수치 안정성. 보통 변경 불필요</td></tr>
    <tr><td><code>weight_decay</code></td><td>0.01</td><td>$\lambda$</td><td>0.01~0.1. 과적합 방지</td></tr>
  </tbody>
</table>

<h2>메모리 사용량</h2>
<div class="formula">$$\text{Optimizer States}=\underbrace{4|\theta|}_{\text{m (FP32)}}+\underbrace{4|\theta|}_{\text{v (FP32)}}=8|\theta|\text{ bytes}$$</div>
<p class="small">예: 1B 파라미터 → 8GB optimizer states</p>

<h2>변형</h2>
<table>
  <thead><tr><th>변형</th><th>차이점</th><th>코드베이스</th></tr></thead>
  <tbody>
    <tr><td><strong>AdamW8bit</strong></td><td>m, v를 8-bit 동적양자화. 메모리 ~2N</td><td>sd-scripts, diffusion-pipe</td></tr>
    <tr><td><strong>PagedAdamW</strong></td><td>가상 메모리 페이징으로 OOM 방지</td><td>sd-scripts (bitsandbytes)</td></tr>
    <tr><td><strong>AdamW (optimi)</strong></td><td>Kahan Summation → BF16 누적 오차 보정</td><td>diffusion-pipe</td></tr>
    <tr><td><strong>StableAdamW</strong></td><td>분산 안정화</td><td>diffusion-pipe</td></tr>
    <tr><td><strong>AdamW8bitKahan</strong></td><td>8-bit + Kahan + 선택적 StableAdamW 모드</td><td>diffusion-pipe</td></tr>
  </tbody>
</table>

<h2>코드매핑</h2>
<div class="codebox"># sd-scripts: library/train_util.py
optimizer = torch.optim.AdamW(
    params, lr=args.learning_rate,
    betas=(0.9, 0.999), weight_decay=0.01, eps=1e-8
)

# diffusion-pipe: train.py → create_optimizer()
if optimizer_type == 'adamw':
    optimizer = torch.optim.AdamW(params, **optimizer_kwargs)

# DiffSynth-Studio: diffsynth/diffusion/runner.py
optimizer = torch.optim.AdamW(
    trainable_params, lr=learning_rate, weight_decay=weight_decay
)</div>

<h2>직관</h2>
<div class="box">
<p>AdamW는 모든 diffusion 훈련의 <strong>기본 선택</strong>입니다. 1st moment ($m$)는 SGD의 모멘텀에 해당하고, 2nd moment ($v$)는 파라미터별 학습률을 적응적으로 조절합니다.</p>
<p><strong>Decoupled WD의 의미:</strong> 원래 Adam에서는 L2 정규화가 그래디언트에 섞여 $v$에 영향을 줬지만, AdamW는 WD를 분리하여 적응적 학습률을 왜곡하지 않습니다.</p>
<p>$\beta_2=0.999$는 약 1000 step의 이동평균을 의미하므로, "최근 1000 step의 기울기 크기 평균"으로 학습률을 조절한다고 생각하면 됩니다.</p>
</div>
</article></div></body></html>
